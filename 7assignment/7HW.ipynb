{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7\n",
    "\n",
    "Delelop language model, which generates death metal band names.  \n",
    "You can get data from https://www.kaggle.com/zhangjuefei/death-metal.  \n",
    "You are free to use any other data, but the most easy way is just to take the band name column.\n",
    "\n",
    "Your language model should be char-based autogression RNN.  \n",
    "Text generation should be terminated when either max length is reached or terminal symbol is generated.  \n",
    "\n",
    "\n",
    "Different band names can be generated by:  \n",
    "1. init $h_0$ as random vector from some probabilty distribution.\n",
    "2. sampling over tokens at each timestep with probability = softmax \n",
    "\n",
    "Calculate perplexity for your model = your objective quality metric.  \n",
    "Also, sample 10 band names from your model for subjective evaluation. E.g. names like 'qwiouefiou23riop2h3' or 'death death death!' are bad examples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tt\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import string\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        batch_size = input.size(0)\n",
    "        encoded = self.encoder(input)\n",
    "        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
    "        output = self.decoder(output.view(batch_size, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(tt.zeros(self.n_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text, len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tensor(string):\n",
    "    tensor = tt.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        try:\n",
    "            tensor[c] = all_characters.index(string[c])\n",
    "        except:\n",
    "            continue\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Делим выборку на тренировочную и валидационную"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling data...\n",
      "Writting data...\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "df = pd.read_csv('bands.csv')\n",
    "groups = list(df['text'])\n",
    "print('Shuffling data...')\n",
    "shuffle(groups)\n",
    "print('Writting data...')\n",
    "with open('bands.txt', 'a') as file:\n",
    "    for group in groups:\n",
    "        file.write(group + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bands.txt', 'r') as file, open('train_bands.txt', 'a') as file_w1, open('valid_bands.txt', 'a') as file_w2:\n",
    "    lines = file.readlines()\n",
    "    sep = int(0.9 * len(lines))\n",
    "    i = 0\n",
    "    for line in lines:\n",
    "        i += 1\n",
    "        if i < sep:\n",
    "            file_w1.write(line)\n",
    "        else:\n",
    "            file_w2.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "batch_size = 32\n",
    "chunk_len = 200\n",
    "decoder = MyModel(\n",
    "    n_characters,\n",
    "    hidden_size,\n",
    "    n_characters\n",
    ")\n",
    "optimizer = tt.optim.Adam(decoder.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(name):\n",
    "    save_filename = os.path.splitext(os.path.basename(name))[0] + '.pt'\n",
    "    tt.save(decoder, save_filename)\n",
    "    print('Saved as %s' % save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train, file_train_len = read_file('train_bands.txt')\n",
    "file_valid, file_valid_len = read_file('valid_bands.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_set(chunk_len, batch_size, file, file_len):\n",
    "    inp = tt.LongTensor(batch_size, chunk_len)\n",
    "    target = tt.LongTensor(batch_size, chunk_len)\n",
    "    for bi in range(batch_size):\n",
    "        start_index = random.randint(0, file_len - chunk_len)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = file[start_index:end_index]\n",
    "        inp[bi] = char_tensor(chunk[:-1])\n",
    "        target[bi] = char_tensor(chunk[1:])\n",
    "    inp = Variable(inp)\n",
    "    target = Variable(target)\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(x):\n",
    "    return 2**x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_epoch(inp, target, model, optimizer, criterion, curr_epoch):\n",
    "\n",
    "    decoder.train()\n",
    "    hidden = decoder.init_hidden(batch_size)\n",
    "    decoder.zero_grad()\n",
    "    running_loss = 0\n",
    "    perplexities = []\n",
    "    \n",
    "    for c in range(chunk_len):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, hidden = decoder(inp[:,c], hidden)\n",
    "        loss = criterion(output.view(batch_size, -1), target[:,c])\n",
    "        perplexities.append(perplexity(loss.item()))\n",
    "        \n",
    "        curr_loss = loss.data.cpu().detach().item()\n",
    "        loss_smoothing = c / (c+1)\n",
    "        running_loss = loss_smoothing * running_loss + (1 - loss_smoothing) * curr_loss\n",
    "    \n",
    "    PERPLEXITY = np.mean(perplexities)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return running_loss, PERPLEXITY\n",
    "\n",
    "def _test_epoch(inp, target, model, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    hidden = decoder.init_hidden(batch_size)\n",
    "    loss = 0\n",
    "    perplexities = []\n",
    "    \n",
    "    with tt.no_grad():\n",
    "        for c in range(chunk_len):\n",
    "            output, hidden = decoder(inp[:,c], hidden)\n",
    "            loss = criterion(output.view(batch_size, -1), target[:,c])\n",
    "            perplexities.append(perplexity(loss.item()))\n",
    "            epoch_loss += loss.data.item()\n",
    "    PERPLEXITY = np.mean(perplexities)\n",
    "    \n",
    "    return epoch_loss / chunk_len, PERPLEXITY\n",
    "\n",
    "\n",
    "def nn_train(model, criterion, optimizer, n_epochs=100, scheduler=None, early_stopping=0):\n",
    "\n",
    "    prev_loss = 100500\n",
    "    es_epochs = 0\n",
    "    best_epoch = None\n",
    "    history = pd.DataFrame()\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        train_loss, train_per = _train_epoch(*random_training_set(300, batch_size, file_train, file_train_len),\n",
    "                                             model, optimizer, criterion, epoch)\n",
    "        valid_loss, valid_per = _test_epoch(*random_training_set(300, batch_size, file_valid, file_valid_len),\n",
    "                                            model, criterion)\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        if epoch % 100 == 0 or epoch == n_epochs-1:\n",
    "            print('|| Epoch %s | Valid loss %.5f | Train loss %.5f | Valid perplexity %.5f | Train perplexity %.5f ||' % (str(epoch),\n",
    "                                                                                                                          valid_loss,\n",
    "                                                                                                                          train_loss,\n",
    "                                                                                                                          train_per,\n",
    "                                                                                                                          valid_per))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf79649fbcd46d294916464fc2f5f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| Epoch 0 | Valid loss 2.69484 | Train loss 2.63052 | Valid perplexity 6.33457 | Train perplexity 6.62086 ||\n",
      "|| Epoch 100 | Valid loss 2.68137 | Train loss 2.68046 | Valid perplexity 6.53741 | Train perplexity 6.52927 ||\n",
      "|| Epoch 200 | Valid loss 2.63052 | Train loss 2.61395 | Valid perplexity 6.25979 | Train perplexity 6.31929 ||\n",
      "|| Epoch 300 | Valid loss 2.65130 | Train loss 2.67726 | Valid perplexity 6.52089 | Train perplexity 6.40427 ||\n",
      "|| Epoch 400 | Valid loss 2.69372 | Train loss 2.66897 | Valid perplexity 6.49674 | Train perplexity 6.61305 ||\n",
      "|| Epoch 500 | Valid loss 2.66746 | Train loss 2.64441 | Valid perplexity 6.37464 | Train perplexity 6.47759 ||\n",
      "|| Epoch 600 | Valid loss 2.58373 | Train loss 2.57041 | Valid perplexity 6.03143 | Train perplexity 6.11563 ||\n",
      "|| Epoch 700 | Valid loss 2.67036 | Train loss 2.62570 | Valid perplexity 6.28575 | Train perplexity 6.48682 ||\n",
      "|| Epoch 800 | Valid loss 2.70903 | Train loss 2.71940 | Valid perplexity 6.71711 | Train perplexity 6.67982 ||\n",
      "|| Epoch 900 | Valid loss 2.66537 | Train loss 2.65747 | Valid perplexity 6.43709 | Train perplexity 6.45729 ||\n",
      "|| Epoch 999 | Valid loss 2.63016 | Train loss 2.69767 | Valid perplexity 6.59860 | Train perplexity 6.29761 ||\n",
      "Saved as bands.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type MyModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "nn_train(decoder, criterion, optimizer, n_epochs=1000)\n",
    "save('bands')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерируем названия групп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(decoder, prime_str='\\n', predict_len=30, temperature=0.8):\n",
    "    hidden = decoder.init_hidden(1)\n",
    "    prime_input = Variable(char_tensor(prime_str).unsqueeze(0))\n",
    "    predicted = ''\n",
    "\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[:,p], hidden)\n",
    "        \n",
    "    inp = prime_input[:,-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = tt.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        predicted_char = all_characters[top_i]\n",
    "        if predicted != '' and predicted_char == '\\n':\n",
    "            break\n",
    "        else:\n",
    "            predicted += predicted_char\n",
    "            inp = Variable(char_tensor(predicted_char).unsqueeze(0))\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inencastarg\n",
      "Folemioss\n",
      "Thorsed\n",
      "Bed\n",
      "Nacinata\n",
      "Ungar\n",
      "Teded\n",
      "Unidgel\n",
      "Wiscid\n",
      "Ferios\n"
     ]
    }
   ],
   "source": [
    "filename = 'bands.pt'\n",
    "decoder = tt.load(filename)\n",
    "\n",
    "for x in range(10):\n",
    "    print(generate(decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
